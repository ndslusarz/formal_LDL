
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}

\usepackage{tcolorbox}
\usepackage{local-macros}
\usepackage{syntax}
\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Placeholder title: Formal methods in constraint-based learning} %TODO Please add

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Jane {Open Access}}{Dummy University Computing Laboratory, [optional: Address], Country \and My second affiliation, Country \and \url{http://www.myhomepage.edu} }{johnqpublic@dummyuni.org}{https://orcid.org/0000-0002-1825-0097}{(Optional) author-specific funding acknowledgements}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional. Use additional curly braces to indicate the correct name splitting when the last name consists of multiple name parts.

%\author{Joan R. Public\footnote{Optional footnote, e.g. to mark corresponding author}}{Department of Informatics, Dummy College, [optional: Address], Country}{joanrpublic@dummycollege.org}{[orcid]}{[funding]}

\authorrunning{running author} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{copyright} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\ccsdesc[100]{\textcolor{red}{Replace ccsdesc macro with valid one}} %TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Dummy keyword} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%COMMENT MACROS
\newtcolorbox{authorComment}[1]{colback=#1}
\newcommand{\natalia}[1]{\begin{authorComment}{red!15}Natalia: #1\end{authorComment}}

\def\rei#1{{\color{red}[NB(rei):#1]}}

\begin{document}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
Abstract.
\end{abstract}


\section{Introduction}

Using Interactive Theorem Provers (ITPs) in combination with machine learning (ML) is a very new approach dating only a few years back CITE. 
The integration of formal methods with machine learning poses many challenges, stemming from the inherent differences between the two domains, from the black-box nature of many ML models, through scalability issues especially in the case of neural networks (NNs) to the lack of well-defined specifications required in formal methods. 

However in the recent years there has been a growing interest in the intersection of these two fields. One of the first attempts to use Coq in conjunction with machine learning is MLCert~\cite{bagnall2019certifying}, a system for mechanized proofs of the generalisation bounds of an ML model in Coq, providing bounds on expected error. MLCert is compatible with deep learning frameworks such as TensorFlow but the classifier does need to be encoded by hand. Coq can be easier used for reasoning about well defined smaller machine learning algorithms, for example for proving convergence of a one-layer perceptron in specific conditions~\cite{murphy2017verified}.

The existing work is not limited to Coq. It has been demonstrated that a neural network can be encoded in a theorem prover, in this case Isabelle/HOL, in order to prove its safety \cite{brucker2023verifying}. Vehicle, a tool for enforcing specifications on neural networks, is also capable of exporting a verified specification to Agda to enable reasoning about the network when considering safety of a larger system\cite{daggitt2023compiling}.

The existing attempts at combining ITPs and machine learning have encountered some of the same problems that originate form the inherent nature of machine learning as a field and are challenging for ITPs. Among the most theoretical ones are the necessity to deal with real domain as well as vectors in order to reason about data as well as need for support for linear algebra and certain areas of calculus. This mathematical complexity is inherent especially to neural networks but is widely known to pose a challenge to formalisation tools. Lastly there is also an issue of scale when formalising large models.

There are many aspects of neural networks that can be formalised. \natalia{Need to find a nice transition in this paragraph}In particular one we investigate in this work is differentiable logics.

Differentiable logics belong to a method of property-based training of neural networks. The idea behind differentiable logic is to use a property that is desirable in a neural network and use it to generate a custom property based loss function. A standard \emph{loss function} $\losssymbol: \Real^n \times \Real^m \rightarrow \Real$ given a pair $(\x, \y)$ of input and desired output, a neural network $f$, calculates how much $f(\x)$ differs from the desired output $\y$ \cite{wang2020comprehensive}.

A property based loss will instead aim to penalise the network for deviating from said property. Training a neural network with a combination of a property based loss function and a standard data-driven one will result in a network trained to fit both the data and the property. 

A DL itself consists of syntax - the logical language used to express properties - and semantics - the "translation" from the syntax to a loss function. A general typed syntax and semantics for expressing different popular DLs that we use as a reference point is Logic of Differentiable Logics (LDL) CITE. The syntax of LDL is a typed first order logic language with addition of lambdas however in this paper we will consider its propositional fragment which can be seen in Figure \ref{fig:syntax}. Importantly the type system includes the existance of vectors, vector indicies and real numbers.

The syntax of LDL is then interpreted into a loss function. The semantics are parametric based on the choice of interpretation for comparisons and logical connectives which differ between various differentiable logics.

\natalia{Let me know if I should expand on LDL here or in the background section instead. Running example might be good to illustrate it since I think this explanation alone is likely confusing but I don't know what sort of running example would be compelling for ITP audience}

\begin{figure}[t]
	\small
	%\footnotesize
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\begin{grammar}
			<expr> $\ni e$ ::=  
			$\identClass$ | $\netClass$ | \elReal $\: \in \Real$ | \elNat $\: \in \Nat$ | \elBool $\: \in \Bool$
			\alt \App{\exprClass}{\exprClass} | \Lam{\identClass}{\typeClass}{\exprClass} | \Let{\identClass}{\typeClass}{\exprClass}{\exprClass}
			\alt \AndSymbol{} | \OrSymbol{} | \NotSymbol{} | \ImpliesSymbol{} | \AddSymbol{} | \NegSymbol{} | \MulSymbol{}
			\alt \NeqSymbol{} | \LeqSymbol{} | \GeqSymbol{} | \LeSymbol{} | \GeSymbol{} | \EqSymbol{}
			\alt \Seq{\exprClass}{\exprClass} | \AtSymbol{}
%			\alt \Forall{\identClass}{\typeClass}{\exprClass} | \Exists{\identClass}{\typeClass}{\exprClass} 

		\end{grammar}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.5\textwidth}
		\begin{grammar}
			<type> $\ni \tau$  ::= 
			\FunType{s}{\typeClass} 
			| s
			
			<simple type> $\ni s$ ::= 
			\alt \BoolType
			\alt \RealType
			\alt \VecType{n}   |  \FinType{n}  \hspace{3em} for $n \in \Nat$
		\end{grammar}
	\end{subfigure}
	\setlength{\belowcaptionskip}{-15pt} 
	\caption{Specification Language of LDL: expressions and types. % (e.g. $x \leq y$ is syntactic sugar for $\leq x\ y$).
	}
	\label{fig:syntax}
\end{figure}

%Furthermore we believe that using formal methods approaches can be beneficial not only in verification, but also in helping to deepen the understanding of machine learning as well as enforcing a more formal approach to more mathematical areas of machine learning. 
%For future programming languages created for AI verification  proofs may, in the future, become compulsory, enforcing an additional level of rigour~\cite{FoMLAS2023:Vehicle_Tutorial_Neural_Network,seshia2022toward}.

The problem that we address is that DL formalisation papers are often unclear or wrong in claims about certain properties of DLs they claim CITE MYSELF, STL. The approach we use is formalisation of the the meta-language LDL in the Coq proof assistant. While as mentioned the mathematical context needed to reason about DLs is challenging for theorem provers, we believe that there exist current state of the art formalisation tools which are expressive enough and we use paper as demonstration that current state of the art already allows for application of formal approaches to well-defined problems in machine learning. 

Hence we have chosen to use Mathematical Components (MathComp)~\cite{mathcomp} in Coq CITE which stands as a testament to the expressive power of formal proof systems in addressing advanced mathematical concepts. At its core, MathComp leverages the Calculus of Inductive Constructions (CIC), the underlying type theory of Coq, to encode mathematical abstractions in a way that allows bridging the gap between mathematics and the formalization of machine learning concepts. 


Our first contribution is the formalisation of several best known DLs using Coq and MathComp. Our formalisation of the LDL framework is designed to be generic and high-level so that it allows for easy extension to other DLs. EXPAND

Secondly, our implementation of the syntax is non-trivial. It utilises dependent types interestingly. We have discovered some unintended behaviours in Coq implementation of dependent types which caused standard tactics to fail in proofs. \natalia{Should I explain the problem with R as context vs variable here, or in some of the later chapters?}

Another of our contributions is demonstrating the capabilities of MathComp, including latest additions to the library CITE, when applied to machine learning. This is the first successful attempt at formalisation of the LDL calculus which includes dealing with concepts known to be problematic in formalisation and yet prevalent in machine learning such as working in the real domain and dealing with vectors.

Lastly we provide formal proofs of theorems that were not rigorously proven in ML literature CITE MYSELF ALSO STL. This refers to both theorems that were stated incorrectly, not listing all of the necessary assumptions as well as incorrect claims about certain DLs holding specific desirable mathematical properties. 
We would like this work to inform design choices in DLs and help fix preexisting issues. We believe that using formal methods approaches can be beneficial not only in verification, but also in helping to deepen the understanding of machine learning as well as enforcing a more formal approach to more mathematical areas of machine learning. \natalia{Should I go into detail here what the actual issue was or should that be left for later? Currently plan to discuss the STL paper shadow-lifting/assoc/idempotence example in later chapter as per plan}


\section{Related work}

To the best of our knowledge there is very little work that is closely related to our approach and involves applying formal methods and ITPs to the training approaches itself instead of an already trained machine learning model or its properties. Furthermore majority of approaches involving formal methods and machine learning  apply to already trained machine learning models, and not approaches such as constraint-based training itself and thus are not very closely related to this work. This area of research is much less explored as of yet, primarily due the difficulty of expressing the mathematical theory underlying neural networks. 

Relevant work dealing with already trained models does involve encoding trained networks in ITPs, such as Isabelle/HOL \cite{brucker2023verifying}. This Isabelle prototype supports importing feedforward neural networks from TensorFlow, eliminating the need to embed each network by hand and it is the first formalisation of neural networks in an interactive theorem prover. Similiarly another recent work to set up other interactive theorem provers for verification of neural network by formalising piecewise affine activation functions in Coq~\cite{aleksandrov2023formalizing}. 

Other works focus on different properties important to the machine learning community such as generalisation, providing formal guarantees of the degree to which the train network will generalise to new data~\cite{bagnall2019certifying}. Since scale and lack of precise mathemtaical definitions are some of the main issues for theorem provers, reasoning about properties of certain smaller but well defined machine learning algorithms lend themselves better to formalisation. We can take as an example proving convergance of a as single-layer perceptron~\cite{murphy2017verified}.

While they do not directly formalise networks, there has also been some research related to algorithms, instead of trained models such as formalisation of an embedding of linear temporal logic and an associated evaluation function in Isabelle to prove its soundness - and furthermore are able to produce OCaml versions of the aforementioned functions that can be integrated with Python and PyTorch eliminating many risks associated with ad-hoc implementations\cite{chevallier2022constrained}. Another related example are formalisation of other types of algorithms~\cite{daukantas2021trimming} or formalisation of neural networks using Haskell~\cite{xie2023haskell}. They, while not directly related to the subject matter, utilise similar approaches and also have to contend with similar issues related to mathematical complexity.

Lastly, a larger but less directly related area area where more broadly understood formal methods and machine learning combine is verification. This field has been rapidly gaining traction in the last few years and there are many different approaches present in neural network verification\cite{urban2021review,9842406,easterbrook1998formal} including abstract interpretation, SMTs (Satisfiability Modulo Theory) \cite{katz2019marabou} solvers or MILP (Mixed Integer Linear Programming). There has also been similar work done for other machine learning approaches than neural networks such as for support vector machines (SVMs)~\cite{ranzato2019robustness} or Decision Tree Ensembles~\cite{einziger2019verifying} though we do not focus on such models in our work. 




\section{Background - notes}


Notes on what probably needs to make its way into background (non-exhaustive)
\begin{itemize}
	\item A good explanation of DL and similar methods in machine learning. Including syntax, semantics in tables. Also explanation of loss functions and their role in training.
	\item Possibly properties here, not later.
\end{itemize}






\section{Temporary Name: Formalisation - notes}

General notes to self on formalisation.

Different functions, lemmas, and things that perhaps stand out.

\subsection{Syntax and semantics}

Implementation of the syntax: $expr$ with it's custom dependant type $simple\_type$. Since the type system prevents non-termination of the lambda expressions they, together with let statements, have been omitted in the Coq formalisation (argumentation why it's okay goes here).



Implementation of the semantics: 
\begin{itemize}
	\item $type\_translation$ semantics of types
	\item a word on translation of indices and vectors using tuples
	\item mention Real library from MathComp to handle reals
	\item $translation$ - a semantics for all fuzzy DLs. Here there needs to be explanation maybe of how DL2 and STL are separate from the fuzzy logics as they are different enough to warrant a separate definition and lemmas, not enough overlap
	translation itself is dependently typed, handles 4 different fuzzy logics through use of global variable (for now it's global variable, up for discussion)
	
\end{itemize}

No quantifiers in the semantics. That does not impact the formalisation of most properties aside from soundness as the other properties do not concern quantifiers.

\subsection{Soundness (and associated lemmas)}

\begin{itemize}
	\item $expr\_ind'$ custom induction principle was needed due to use of sequences. Explain sequences in MathComp (hidden tuples)? Explain that this is needed due to possibility of non-associative binary operations in the language.
	\item $[name of DL]\_translate\_Bool\_T\_01$ - lemmas that prove that the fuzzy DL semantics fall within the range [0,1]
\end{itemize}

Inversion lemmas also mentioned here possibly.

\subsection{Logical properties}

On associativity and commutativity for the DLs that have them (and that associativity is much more tricky then commutativity to prove). Not entirely sure if anything more interesting is here specifically so likely not its own subsection.

\subsection{Shadow-lifting}

Using limit definition of partial differentiation.

It does work out of the box using MathComp in that limits exist but does require more lemmas as it is a new library.
Just a lot more detail on this proof when finished.


\section{Other notes}

Complexity increase depending on DL is. While ``complexity'' is not easy to measure for a DL on paper it is obvious in its impact in formalisation. Associativity of conjunction for Åukawsiewic, Godel and product all have around 10 lines (get exact after cleanup) - but Yager takes 38 (again, correct after cleanup). If I prove STL mention that (if case split in analogous ways to other proofs we get 38 cases)

A lot of difficulty is added due to using nary operations for the sake of generality (bigops, etc.). Will need to have good reasoning for that (primarily htat it's following the very general idea behind LDL).


\section{Shadow lifting, idempotence and associativity}

The claim that shadow lifting and associativity cannot hold together is inaccurate, we need idempotence too.

To make this formal, let us assume a generic semantic interpretation of and (f), with the following properties.

[[ p /\ q ]] = f [[ p ]]  [[ q ]] (it's structural)

I: [[p /\ p]] = [[p]] (has idempotence)

A: [[p /\ (q /\ r)]] = [[(p /\ q) /\ r]] (has associativity)
 
(has shadow lifting)
SL1: d [[p /\ q]] / d p > 0
SL2: d [[p /\ q]] / d q > 0


[[ 1 /\ (1 /\ 1.1) ]] =? [[ 1 /\ (1.1 /\ 1.1) ]]

[[ 1 /\ (1 /\ 1.1) ]] =A [[ (1 /\ 1) /\ 1.1 ]] =I [[ 1 /\ 1.1 ]] =I [[ 1 /\ (1.1 /\ 1.1) ]]



[[ 1 /\ 1.1 ]] <SL1 [[ 1.1 /\ 1.1 ]]
->
[[ 1 /\ (1 /\ 1.1) ]] <SL2 [[ 1 /\ (1.1 /\ 1.1) ]]


%%
%% Bibliography
%%

%% Please use bibtex, 

\bibliography{bibliography}

\appendix

\end{document}
